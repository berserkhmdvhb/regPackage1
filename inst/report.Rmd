---
title: "Car Insurance Claims Classification Report"
author: "Hamed Vaheb"
#1date: "09 Jan 2021"
output:
  html_document:
    number_sections: yes
    toc: yes
    code_folding: hide
    theme: readable
    highlight: haddock
  pdf_document:
    toc: yes
---



```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(regPackage1)
library(dplyr)
library(janitor)
library(tidymodels)
library(visdat)
library(ggplot2)
library(imbalance)
library(readr)
library(caret) #for dummyVar function
library(Metrics)
```
# Introduction
This report is dedicated to project of a workshop of master of data science at University of Luxembourg, I built the [`regPackage1`](https://github.com/berserkhmdvhb/regPackage1). 
You can install this package by first installing the [devtools](https://www.r-project.org/nosvn/pandoc/devtools.html) package and then using the following syntax:

`devtools::install_github("https://github.com/berserkhmdvhb/regPackage1")`

You can explore the documentation of package I provided with 

`help(package="regPackage1")`

Also, this report is accessible as vignette of the package, which can be accessed with

`vignette(package = "regPackage1")`

(Note: I just made a meta-reference)


Note that all functions of the package are suffixed with `hmd`.


 
# Load Dataset
To provide a test case on a dataset with discrete target variable, I used the the [car insurance data](https://www.kaggle.com/datasets/sagnik1511/car-insurance-data).

The dataset is embedded in the package, therefore by simply running the following commands one can load and display the dataset.
. It can be used by the following syntax:

`data("car_insurance_data")`




```{r}
data(car_insurance_data)

dplyr::glimpse(car_insurance_data)
```


Summary of statistical properties of columns are provided in the following:

```{r}
summary(car_insurance_data)
```




# Preprocess {#preprocess}
Before feeding data to the model, various preprocessing stages are performed in the following subsections:

## Clean Column Names

```{r}
df <- janitor::clean_names(car_insurance_data)

names(df)
```

And the following is to make the codes reproducible, as random values might be involved in different stages:
```{r}
set.seed(12345)
```

## Categorical Columns
We need to factorise the categorical columns, which can be done by merely inputting columns intended to be converted in `cat_cols` in the `prepare_hmd` function. Before doing that, we report and visualise the nature of categorical data and their frequency in the dataset

```{r}
categoricals_print_hmd(df)
```

```{r warning=FALSE}
vis_dat(df)
```

```{r}
df <- categoricals_hmd(df) 
dplyr::glimpse(df)
```


```{r}
vis_dat(df)
```


## Missing Data



```{r}
df[is.na(df) | df=="Inf"] = NA
vis_miss(df, sort_miss = TRUE)
```


```{r}
is.na(df |> select("credit_score","annual_mileage")) |> colSums()
## Classification Models
```

As only two numerical columns include NA values, the will be imputed with the median of their respective columns.
For this `impute_median_hmd` function is used from the package:

```{r}
df <- impute_median_hmd(df)
is.na(df) |> colSums()
```
As evident, there is no missing data anymore.
## Check Imbalance of Target

**Visualize Balance**

As imbalance of target classes affect predictions, the frequency of each class in the `outcome` column is visualized:


```{r}
# Most basic bar chart
ggplot(df, aes(x = outcome)) +
    geom_bar()
```

```{r}
table(df$outcome)
```

```{r}
imbalance::imbalanceRatio(df, classAttr = "outcome")
```
## Encoding Categorical Columns

Since later it is realized that data is imbalanced and therefore it should be resampled, it is required that data would only contain numerical columns for the next part. For this reason, one-hot-encoding is applied in the following:

```{r}
dummy <- dummyVars(" ~ .", data=df)
df_enc <- data.frame(predict(dummy, newdata = df)) 
dplyr::glimpse(df_enc)
```

Ensuring the encoded dataframe didn't slip a NA value:
```{r}
is.na(df_enc) |> colSums()
```

## Split to Train/Test



```{r}

df_dict <- train_test_splitter_hmd(df_enc, 
                             proportion=0.8)
train <- df_dict$train
test <- df_dict$test


```

## Normalize

```{r}

for (col in names(train)){
  if (col %in% c("annual_mileage", "postal_code", "credit_score"))
  {
    next
  }
  train[[col]] <- as.integer(train[[col]])
  test[[col]] <- as.integer(test[[col]])
}
```




```{r}
df_dict_norm <- normalizer_hmd(train,test)


train <- df_dict_norm$train_norm
test <- df_dict_norm$test_norm

train$id <- as.double(train$id)
test$id <- as.double(test$id)
```

```{r}
print(paste("train data has", nrow(train), "rows and", ncol(train), "columns"))
print(paste("test data has", nrow(test), "rows and", ncol(test), "columns"))
```

## Oversampling


```{r}
#X <- df_enc[, colnames(df_enc)[colnames(df_enc) != #'outcome']]
#y <- df_enc[['outcome']]

#train_racog <- racog(train, 
#                     numInstances = 3000, 
#                     burnin = 100, 
#                     lag = 20, 
#                     classAttr = "outcome")
```





Merge new sampled data with original train dataset

```{r}
#insurance_train <- rbind(train , train_racog)
```
 


Save the processed dataframe to memory:
  
  
  


```{r}
#path = "/home/hamed/Documents/R/regPackage1/inst/"
#file_name = "insurance_train.csv"
#readr::write_csv(insurance_train, 
#                 file = #paste(path,file_name,sep=""))
```


Load back the processed data

```{r}
#insurance_train <- #readr::read_csv(paste(path,file_name,sep=""))
```

Do the same for test data

```{r}
#insurance_test <- test
#path = "/home/hamed/Documents/R/regPackage1/inst/"
#file_name = "insurance_test.csv"
#readr::write_csv(insurance_test, 
#                 file = paste(path,file_name,sep=""))
                   
#insurance_test <- #readr::read_csv(paste(path,file_name,sep=""))
```


we need to also remove the id of the beneficiaries as it will bias the learning towards this and may cause data leakage for test datasets with the same id

```{r}
#insurance_train <- within(insurance_train, rm("id"))
#insurance_test <- within(insurance_test, rm("id"))
```

```{r}
data("insurance_train")
data("insurance_test")
```



```{r}
glimpse(insurance_train)
```

```{r}
glimpse(insurance_test)
```
```{r}
insurance_train$outcome <- as.factor(insurance_train$outcome)
insurance_test$outcome <- as.factor(insurance_test$outcome)
```

```{r}
table(insurance_train$outcome)
```





I added a `preprocess_hmd` function in package that is a wrapper around all the steps and functions from the [preprocessing section](#preprocess)
section. Therefore, simply by writing the following syntax, the final `insurance_train` and `insurance_tetst` datasets can be obtained from the original `car_insurance_data`:

```{r}
#h <- preprocess_hmd(car_insurance_data)
#insurance_train <- h$insurance_train
#insurance_test <- h$insurance_test
```






Data Visualisation


```{r}
#insurance_train |> ggplot(aes(x = credit_score, y = #speeding_violations)) +
#  geom_point(color = outcome)
```

# Classification Models

## Generalized Linear Models

```{r}
actual <- insurance_test$outcome
```

**GLMNET**

```{r}
fit <- glmnet_fit_hmd(insurance_train, target="outcome", family="binomial")
h <- glmnet_predict_hmd(fit, 
                        data = insurance_test,  
                        target = "outcome", 
                        type = "response")
coef = h$coef
pred_glm <- h$predictions
pred_proba_glm <- h$predict_proba
```

```{r}
summary(fit)    
```

```{r}
coef
```

```{r}
head(pred_glm)
```

Extract AIC and BIC from the fit of glm
```{r}
fit$aic
```


**cross validated GLMNET**


```{r}
fit <- glmnet_cv_fit_hmd(insurance_train, target="outcome", family="binomial")
h <- glmnet_cv_predict_hmd(fit, 
                           data = insurance_test, 
                           target = "outcome", 
                           lchoice = "min", 
                           type = "response")
coef = h$coef
pred_glm_cv <- h$predictions
pred_proba_glm_cv <- h$predict_proba
```

```{r}
summary(fit)    
```

```{r}
coef
```

```{r}
head(pred_glm_cv)
```

Extract AIC and BIC from the fit of glm
```{r}
fit$aic
```
### Evaluate


**GLMNET**


Confusion Matrix


```{r}
cf_glmnet <- caret::confusionMatrix(data = as.factor(pred_glm), reference = as.factor(actual))
cf_glmnet
```


```{r}
acc_glmnet <- Metrics::accuracy(as.factor(pred_glm), as.factor(actual))
acc_glmnet
```


```{r}
prec_glmnet <- Metrics::precision(pred_glm, actual)
prec_glmnet
```


```{r}
rec_glmnet <- Metrics::recall(pred_glm, actual)
rec_glmnet
```

```{r}
f1_glmnet <- Metrics::f1(pred_glm, actual)
f1_glmnet
```



**GLMNET CV**


Confusion Matrix


```{r}
cf_glmnet_cv <- caret::confusionMatrix(data = as.factor(pred_glm_cv), reference = as.factor(actual))
cf_glmnet_cv
```


```{r}
acc_glmnet_cv <- Metrics::accuracy(pred_glm_cv, actual)
acc_glmnet_cv
```


```{r}
prec_glmnet_cv <- Metrics::precision(pred_glm_cv, actual)
prec_glmnet_cv
```


```{r}
rec_glmnet_cv <- Metrics::recall(pred_glm_cv, actual)
rec_glmnet_cv
```

```{r}
f1_glmnet_cv <- Metrics::f1(pred_glm_cv, actual)
f1_glmnet_cv
```



